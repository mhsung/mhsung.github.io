---
layout: page

title:  "DeepMetaHandles: Learning Deformation Meta-Handles of 3D Meshes with Biharmonic Coordinates"

author: <a href="http://cseweb.ucsd.edu/~mil070/" target="_blank">Minghua Liu</a>, <a href="http://mhsung.github.io/" target="_blank">Minhyuk Sung</a>, <a href="https://research.adobe.com/person/radomir-mech/" target="_blank">Radomír Měch</a>, <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a>

journal: CVPR 2021 (Oral)

slug: deep-meta-handles

date: 2021-05-15

arxiv-link: https://arxiv.org/abs/2102.09105

code-link: https://github.com/Colin97/DeepMetaHandles
---


## {{ page.title }}
{:.title}
#### {{ page.author }}
{:.title}
#### {{ page.journal }}
{:.title}

<br />
![]({{site.baseurl}}/assets/images/{{page.slug}}-teaser.png)

### Abstract
>We propose DeepMetaHandles, a 3D conditional generative model based on mesh deformation. Given a collection of 3D meshes of a category and their deformation handles (control points), our method learns a set of meta-handles for each shape, which are represented as combinations of the given handles. The disentangled meta-handles factorize all the plausible deformations of the shape, while each of them corresponds to an intuitive deformation. A new deformation can then be generated by sampling the coefficients of the meta-handles in a specific range. We employ biharmonic coordinates as the deformation function, which can smoothly propagate the control points' translations to the entire mesh. To avoid learning zero deformation as meta-handles, we incorporate a target-fitting module which deforms the input mesh to match a random target. To enhance deformations' plausibility, we employ a soft-rasterizer-based discriminator that projects the meshes to a 2D space. Our experiments demonstrate the superiority of the generated deformations as well as the interpretability and consistency of the learned meta-handles.
<br />

*{{page.author}}<br>
**{{page.title}}**<br>
{{page.journal}}*<br>
[arXiv]({{page.arxiv-link}}){:target="_blank"}  | 
[Code]({{page.code-link}}){:target="_blank"}

### Bibtex
```
@proceedings{DeepMetaHandles:2021,
  author = {Liu, Minghua and Sung, Minhyuk and M\v{e}ch, Radom\'{i}r and Su, Hao},
  title = {DeepMetaHandles: Learning Deformation Meta-Handles of 3D Meshes with Biharmonic Coordinates},
  booktitle = {CVPR}, 
  year = {2021}
}
```

### Interactive Shape Editing Demo

**[[Open in a New Window](https://mhsung.github.io/deep-meta-handles-demo/web_demo.html){:target="_blank"}]**

Move the meta-handle sliders on the right panel to edit the shapes jointly.<br>
Drag on shapes to change the viewpoint.<br>
<font color="grey">(It may take 10-20 seconds for loading.)</font>

<p align="center">
<iframe width="100%" height="600px" src="https://mhsung.github.io/deep-meta-handles-demo/web_demo_embed.html" frameborder="0" allowfullscreen></iframe>
</p>


### Meta-Handle Results

**[[Open in a New Window](http://cseweb.ucsd.edu/~mil070/deep_meta_handles_supp_animations.html){:target="_blank"}]**

Each row shows deformations of meta-handles with the same index for different shapes.<br>
Scroll down for more results.

<p align="center">
<iframe width="100%" height="720px" src="https://mhsung.github.io/deep-meta-handles-demo/web_animations_embed.html" frameborder="0" allowfullscreen></iframe>
</p>

#### Acknowledgements
This work is supported in part by gifts from Adobe, Kwai, Qualcomm, and Vivo.

<br />
